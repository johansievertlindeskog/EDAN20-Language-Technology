{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #4: Extracting syntactic groups using machine-learning techniques\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will create a system to extract syntactic groups from a text. You will apply it to the CoNLL 2000 dataset. In addition, you will try to link a few extracted named entities to real things using wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objectives of this assignment are to:\n",
    "* Write a program to detect partial syntactic structures\n",
    "* Extract named entities and link them to real things using Wikipedia\n",
    "* Understand the principles of supervised machine learning techniques applied to language processing\n",
    "* Use a popular machine learning toolkit: scikit-learn\n",
    "* Write a short report of 2 to 3 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a training and a test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As annotated data and annotation scheme, you will use the data available from [CoNLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/).\n",
    "* Download both the training and test sets and decompress them.\n",
    "* Local copies are also available here: [train.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt) and [test.txt](https://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt)\n",
    "* Read the description of the CoNLL 2000 task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to adjust the paths to load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'conll2000/train.txt'\n",
    "test_file = 'conll2000/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the functions below to load the datasets. They store the corpus in a list of sentences. Each sentence is a list of rows, where each row is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file):\n",
    "    \"\"\"\n",
    "    Creates a list of sentences from the corpus\n",
    "    Each sentence is a string\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f = open(file).read().strip()\n",
    "    sentences = f.split('\\n\\n')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rows(sentences, column_names):\n",
    "    \"\"\"\n",
    "    Creates a list of sentence where each sentence is a list of lines\n",
    "    Each line is a dictionary of columns\n",
    "    :param sentences:\n",
    "    :param column_names:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        rows = sentence.split('\\n')\n",
    "        sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "        new_sentences.append(sentence)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 files have three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical algorithms for language processing start with a so-called baseline. The baseline performance corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs.\n",
    "\n",
    "You will implement the baseline proposed by the organizers of the\n",
    "        <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\">CoNLL 2000 shared task</a>, Sect. <i>Results</i>.\n",
    "1. Read it;\n",
    "2. In the report you will tell what do you think of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to count the parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(corpus):\n",
    "    \"\"\"\n",
    "    Computes the part-of-speech distribution\n",
    "    in a CoNLL 2000 file\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['pos'] in pos_cnt:\n",
    "                pos_cnt[row['pos']] += 1\n",
    "            else:\n",
    "                pos_cnt[row['pos']] = 1\n",
    "    return pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first collect all the parts of speech and we count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 30147,\n",
       " 'IN': 22764,\n",
       " 'DT': 18335,\n",
       " 'VBZ': 4648,\n",
       " 'RB': 6607,\n",
       " 'VBN': 4763,\n",
       " 'TO': 5081,\n",
       " 'VB': 6017,\n",
       " 'JJ': 13085,\n",
       " 'NNS': 13619,\n",
       " 'NNP': 19884,\n",
       " ',': 10770,\n",
       " 'CC': 5372,\n",
       " 'POS': 1769,\n",
       " '.': 8827,\n",
       " 'VBP': 2868,\n",
       " 'VBG': 3272,\n",
       " 'PRP$': 1881,\n",
       " 'CD': 8315,\n",
       " '``': 1531,\n",
       " \"''\": 1493,\n",
       " 'VBD': 6745,\n",
       " 'EX': 206,\n",
       " 'MD': 2167,\n",
       " '#': 36,\n",
       " '(': 274,\n",
       " '$': 1750,\n",
       " ')': 281,\n",
       " 'NNPS': 420,\n",
       " 'PRP': 3820,\n",
       " 'JJS': 374,\n",
       " 'WP': 529,\n",
       " 'RBR': 321,\n",
       " 'JJR': 853,\n",
       " 'WDT': 955,\n",
       " 'WRB': 478,\n",
       " 'RBS': 191,\n",
       " 'PDT': 55,\n",
       " 'RP': 83,\n",
       " ':': 1047,\n",
       " 'FW': 38,\n",
       " 'WP$': 35,\n",
       " 'SYM': 6,\n",
       " 'UH': 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cnt = count_pos(train_corpus)\n",
    "pos_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the chunk distribution for each part of speech. You will use the training file to derive the distribution and you will store the results in a dictionary. Below, you have an excerpt of the expected results:\n",
    "```\n",
    "{'JJR':\n",
    "{'I-ADVP': 17, 'I-ADJP': 45, 'I-NP': 204, 'B-ADVP': 63,\n",
    "'B-PP': 2, 'B-ADJP': 111, 'B-NP': 382, 'B-VP': 2,\n",
    "'I-VP': 11, 'O': 16},\n",
    "'CC':\n",
    "{'B-ADVP': 3, 'O': 3676, 'I-VP': 104, 'B-CONJP': 6,\n",
    "'I-ADVP': 30, 'I-UCP': 2, 'I-PP': 24, 'I-ADJP': 26,\n",
    "'I-NP': 1409, 'B-ADJP': 2, 'B-NP': 18, 'B-PP': 70,\n",
    "'I-PRT': 1, 'B-VP': 1},\n",
    "'NN':\n",
    "{'B-LST': 2, 'I-INTJ': 2, 'B-ADVP': 38, 'O': 37,\n",
    "'I-ADVP': 11, 'B-INTJ': 1, 'I-UCP': 2, 'B-UCP': 2,\n",
    "'I-VP': 77, 'B-PRT': 2, 'I-ADJP': 41, 'I-NP': 24456,\n",
    "'B-ADJP': 44, 'B-NP': 5160, 'B-PP': 15, 'B-VP': 257},\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def chunk_dist(corpus):  # ett corpus\n",
    "    chunk_dict = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['pos'] not in chunk_dict:\n",
    "                chunk_dict[row['pos']]  = {}\n",
    "                \n",
    "            if row['chunk'] in chunk_dict[row['pos']]:\n",
    "                chunk_dict[row['pos']][row['chunk']] +=  1\n",
    "            else: \n",
    "                chunk_dict[row['pos']][row['chunk']] = 1 \n",
    "    \n",
    "    return chunk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-NP': 5160,\n",
       " 'I-NP': 24456,\n",
       " 'B-VP': 257,\n",
       " 'B-ADJP': 44,\n",
       " 'B-ADVP': 38,\n",
       " 'O': 37,\n",
       " 'B-PP': 15,\n",
       " 'I-ADVP': 11,\n",
       " 'I-ADJP': 41,\n",
       " 'I-VP': 77,\n",
       " 'B-INTJ': 1,\n",
       " 'B-LST': 2,\n",
       " 'B-UCP': 2,\n",
       " 'I-UCP': 2,\n",
       " 'B-PRT': 2,\n",
       " 'I-INTJ': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkDist = chunk_dist(train_corpus)\n",
    "chunkDist['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the POS-chunk associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each part of speech, select the best association. In the example above, you will have (NN, I-NP) as it is the most frequent. You will store the results in a dictionary that you will call `pos_chunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "pos_chunk = {}\n",
    "for pos in chunkDist:\n",
    "    tp_list = sorted(chunkDist[pos].items(), key=lambda x: x[1], reverse=True) #  sorted returnerar en lista \n",
    "    pos_chunk[pos] = tp_list[0][0]                                        #  av tuple-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I-NP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_chunk['NN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the resulting associations, apply your chunker to the test file. You will write a `predict(model, corpus)` function, where `model` will be your associations and `corpus`, the test corpus. You will format the test corpus as a dictionary, where you will add a `pchunk` key for each row with a value that will correspond to the predicted chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def predict(model, corpus):\n",
    "    bad_pred = corpus\n",
    "    for i in range(len(bad_pred)):\n",
    "        for k in range(len(bad_pred[i])):\n",
    "            bad_pred[i][k]['pchunk'] = model[bad_pred[i][k]['pos']]\n",
    "\n",
    "    return bad_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict the groups. You should have added a `pchunk` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test_corpus = predict(pos_chunk, test_corpus)\n",
    "predicted_test_corpus[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the performance of the baseline with the tag accuracy: the percentage of words that receive the correct tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(predicted):\n",
    "    \"\"\"\n",
    "    Evaluates the predicted chunk accuracy\n",
    "    :param predicted:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_cnt = 0\n",
    "    correct = 0\n",
    "    for sentence in predicted:\n",
    "        for row in sentence:\n",
    "            word_cnt += 1\n",
    "            if row['chunk'] == row['pchunk']:\n",
    "                correct += 1\n",
    "    return correct / word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729066846782194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = eval(predicted_test_corpus)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CoNLL evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very misleading as it is biased by the most frequent tags. It is not a good way to evaluate chunking. Instead, CoNLL computes the F1 score of all the chunks with a specific evaluation script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the CoNLL evaluation script, you will store your results in an output file that has four columns. The three first columns will be the input columns from the test file: \n",
    "* word, \n",
    "* part of speech, and \n",
    "* gold-standard chunk. \n",
    "\n",
    "You will append the predicted chunk as the 4th column. Your output file should look like the excerpt below:\n",
    "```\n",
    "Rockwell NNP B-NP I-NP\n",
    "International NNP I-NP I-NP\n",
    "Corp. NNP I-NP I-NP\n",
    "'s POS B-NP B-NP\n",
    "Tulsa NNP I-NP I-NP\n",
    "unit NN I-NP I-NP\n",
    "said VBD B-VP B-VP\n",
    "it PRP B-NP B-NP\n",
    "```\n",
    "The separator is the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use a `save_results(output_dict, keys, output_file)` function, where the keys will be `['form', 'pos', 'chunk', 'pchunk']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['form', 'pos', 'chunk', 'pchunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_dict, keys, output_file):\n",
    "    f_out = open(output_file, 'w')\n",
    "    # We write the word (form), part of speech (pos),\n",
    "    # gold-standard chunk (chunk), and predicted chunk (pchunk)\n",
    "    for sentence in output_dict:\n",
    "        for row in sentence:\n",
    "            for key in keys:\n",
    "                f_out.write(row[key] + ' ')\n",
    "            f_out.write('\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(predicted_test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CoNLL 2000 evaluation script will use these two last columns, chunk and predicted chunk, to compute the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your results, you have two options:\n",
    "1. Use the original conlleval script here:  <a href=\"https://www.clips.uantwerpen.be/conll2000/chunking/\"><tt>conlleval.txt</tt></a>.\n",
    "2. Use a Python translation of it. \n",
    "\n",
    "You will use the second option and you will describe the results you obtained in your report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Python translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the script with:\n",
    "```\n",
    "pip install conlleval\n",
    "```\n",
    "from https://github.com/kaniblu/conlleval\n",
    "and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conlleval\n",
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "baseline_score = res['overall']['chunks']['evals']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770671072299583"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The official script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to double-check your results with the original CoNLL script. It is more complex to use however:\n",
    "* <tt>conlleval.txt</tt> is the official CoNLL Perl script. It expects the two last columns of the test set to be the manually assigned chunk (gold standard) and the predicted chunk.\n",
    "* <tt>conlleval.txt</tt> was written for Unix and if you run Windows, you will have to use a terminal command. In the File menu of the notebook, select New and then Terminal.\n",
    "* Start it like this: ` $ conlleval.txt <out` where the `out` file contains both the gold and predicted chunk tags. `conlleval.txt` is a Perl script.\n",
    "* Perl is installed on most Unix distributions. If it is not installed on your machine, you need to install it. Make also sure that you have the execution rights. Otherwise change them with: `$ chmod +x conlleval.txt`\n",
    "* The `conlleval.txt` script expects the new lines to be `\\n` as in Unix. If you run your Python program on Windows, your new lines will be `\\r\\n`. To have the correct new lines, add this parameter to `open()`: `newline='\\n’` like this: `f_out = open('out', ‘w’, newline='\\n’)`\n",
    "* The complete description of the CoNLL 2000 evaluation script is available here: [https://www.clips.uantwerpen.be/conll2000/chunking/output.html](https://www.clips.uantwerpen.be/conll2000/chunking/output.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: A first ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will apply and explore a machine-learning program.\n",
    "\n",
    "The program that won the CoNLL 2000 shared task (Kudoh and Matsumoto, 2000) used a window of five words around the chunk tag to identify, $c_i$. They built a feature vector consisting of:\n",
    "1. The values of the five words in this window: $w_{i-2}, w_{i-1}, w_{i}, w_{i+1}, w_{i+2}$\n",
    "2. The values of the five parts of speech in this window: $t_{i-2}, t_{i-1}, t_{i}, t_{i+1}, t_{i+2}$\n",
    "3. The values of the two previous chunk tags in the first part of the window: $c_{i-2}, c_{i-1}$\n",
    "\n",
    "The two last parameters (3.) are said to be dynamic because the program computes them at run-time. Read [Kudoh and Matsumoto's paper](http://www.clips.uantwerpen.be/conll2000/pdf/14244kud.pdf) and the [Yamcha](http://www.chasen.org/~taku/software/yamcha/) software site.\n",
    "\n",
    "You will start with a given code that uses the two first sets of features (1. and 2.) and add yourself the last one (3.) to improve the performance of your chunker. Kudoh and Matsumoto trained a classifier based on support vector machines. You will use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import os\n",
    "import requests\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first function to extract features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_static(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j]['chunk'])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_static(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_static(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the window and the names of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                 'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the corpus and format it as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict, y = extract_features_static(train_corpus, w_size, feature_names)\n",
    "X_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-PP', 'B-NP']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the  TH\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)  #  fit_transform på train data, endast transform på test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the sentences and create a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static(test_corpus, w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-NP', 'I-NP'], dtype='<U7')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted = classifier.predict(X_test)\n",
    "y_test_predicted[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the predicted chunks to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index sould be equal to the length of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47377"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inx)\n",
    "len(y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR', 'pchunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'pchunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP', 'pchunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "simple_ml_score = res['overall']['chunks']['evals']['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157688948047087"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question on the ML program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the feature vector that corresponds to the <tt>ml_chunker.py</tt> program? Is it the same Kudoh\n",
    "    and Matsumoto used in their experiment?\n",
    "2. What is the performance of the chunker?\n",
    "3. Remove the lexical features (the words) from the feature vector and measure the performance. You should\n",
    "    observe a decrease.\n",
    "4. What is the classifier used in the program? \n",
    "5. As an optional task, you may try two other classifiers from sklearn and measure their performance: decision trees, perceptron, support vector machines, etc. Be aware that support vector machines take a long time to train: up to one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning: Adding all the features from Kudoh and Matsumoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement the feature vector used in the previous section with the two dynamic features, $c_{i-2}, c_{i-1}$, and train a new model. You will need to write a new `extract_features_sent_dyn` and `predict` functions. \n",
    "In his experiments, your teacher obtained a F1 score of 92.65 with logistic regression and the default parameters from sklearn, i.e. `linear_model.LogisticRegression()`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A frequent mistake in the labs** is to use the gold-standard chunks from the test set. Be aware that  when you predict the test set, you do not know the dynamic features in advance and you must  not use the ones from the test file. You will use the two previous chunk tags that you have predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to reach a global F1 score of 92 to pass this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write your code here\n",
    "def extract_features_sent_dyn(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = [{'form': 'BOS', 'pos': 'BOS', 'chunk': 'BOS'}]\n",
    "    end = [{'form': 'EOS', 'pos': 'EOS', 'chunk': 'EOS'}]\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    padded_sentence = start + sentence\n",
    "    padded_sentence += end\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['form'].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j]['pos'])\n",
    "        # The chunks (Up to the word)\n",
    "        \n",
    "        for j in range(w_size):\n",
    "            x.append(padded_sentence[i + j]['chunk']) #stod feature_line.append(dict(zip(feature_names, x))) innan\n",
    "        \n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size]['chunk'])\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_dyn(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_dyn(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_dyn = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                     'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2', 'chunk_n2',\n",
    "                     'chunk_n1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = read_sentences(train_file)\n",
    "train_corpus = split_rows(train_sentences, column_names)\n",
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dict, y = extract_features_dyn(train_corpus, w_size, feature_names_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'confidence',\n",
       "  'word_p1': 'in',\n",
       "  'word_p2': 'the',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'DT',\n",
       "  'chunk_n2': 'BOS',\n",
       "  'chunk_n1': 'BOS'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'confidence',\n",
       "  'word': 'in',\n",
       "  'word_p1': 'the',\n",
       "  'word_p2': 'pound',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'BOS',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'confidence',\n",
       "  'word_n1': 'in',\n",
       "  'word': 'the',\n",
       "  'word_p1': 'pound',\n",
       "  'word_p2': 'is',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'DT',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'VBZ',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'B-PP'},\n",
       " {'word_n2': 'in',\n",
       "  'word_n1': 'the',\n",
       "  'word': 'pound',\n",
       "  'word_p1': 'is',\n",
       "  'word_p2': 'widely',\n",
       "  'pos_n2': 'IN',\n",
       "  'pos_n1': 'DT',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'VBZ',\n",
       "  'pos_p2': 'RB',\n",
       "  'chunk_n2': 'B-PP',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'the',\n",
       "  'word_n1': 'pound',\n",
       "  'word': 'is',\n",
       "  'word_p1': 'widely',\n",
       "  'word_p2': 'expected',\n",
       "  'pos_n2': 'DT',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'VBZ',\n",
       "  'pos_p1': 'RB',\n",
       "  'pos_p2': 'VBN',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'I-NP'},\n",
       " {'word_n2': 'pound',\n",
       "  'word_n1': 'is',\n",
       "  'word': 'widely',\n",
       "  'word_p1': 'expected',\n",
       "  'word_p2': 'to',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'VBZ',\n",
       "  'pos': 'RB',\n",
       "  'pos_p1': 'VBN',\n",
       "  'pos_p2': 'TO',\n",
       "  'chunk_n2': 'I-NP',\n",
       "  'chunk_n1': 'B-VP'},\n",
       " {'word_n2': 'is',\n",
       "  'word_n1': 'widely',\n",
       "  'word': 'expected',\n",
       "  'word_p1': 'to',\n",
       "  'word_p2': 'take',\n",
       "  'pos_n2': 'VBZ',\n",
       "  'pos_n1': 'RB',\n",
       "  'pos': 'VBN',\n",
       "  'pos_p1': 'TO',\n",
       "  'pos_p2': 'VB',\n",
       "  'chunk_n2': 'B-VP',\n",
       "  'chunk_n1': 'I-VP'},\n",
       " {'word_n2': 'widely',\n",
       "  'word_n1': 'expected',\n",
       "  'word': 'to',\n",
       "  'word_p1': 'take',\n",
       "  'word_p2': 'another',\n",
       "  'pos_n2': 'RB',\n",
       "  'pos_n1': 'VBN',\n",
       "  'pos': 'TO',\n",
       "  'pos_p1': 'VB',\n",
       "  'pos_p2': 'DT',\n",
       "  'chunk_n2': 'I-VP',\n",
       "  'chunk_n1': 'I-VP'},\n",
       " {'word_n2': 'expected',\n",
       "  'word_n1': 'to',\n",
       "  'word': 'take',\n",
       "  'word_p1': 'another',\n",
       "  'word_p2': 'sharp',\n",
       "  'pos_n2': 'VBN',\n",
       "  'pos_n1': 'TO',\n",
       "  'pos': 'VB',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'JJ',\n",
       "  'chunk_n2': 'I-VP',\n",
       "  'chunk_n1': 'I-VP'},\n",
       " {'word_n2': 'to',\n",
       "  'word_n1': 'take',\n",
       "  'word': 'another',\n",
       "  'word_p1': 'sharp',\n",
       "  'word_p2': 'dive',\n",
       "  'pos_n2': 'TO',\n",
       "  'pos_n1': 'VB',\n",
       "  'pos': 'DT',\n",
       "  'pos_p1': 'JJ',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'I-VP',\n",
       "  'chunk_n1': 'I-VP'},\n",
       " {'word_n2': 'take',\n",
       "  'word_n1': 'another',\n",
       "  'word': 'sharp',\n",
       "  'word_p1': 'dive',\n",
       "  'word_p2': 'if',\n",
       "  'pos_n2': 'VB',\n",
       "  'pos_n1': 'DT',\n",
       "  'pos': 'JJ',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'IN',\n",
       "  'chunk_n2': 'I-VP',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'another',\n",
       "  'word_n1': 'sharp',\n",
       "  'word': 'dive',\n",
       "  'word_p1': 'if',\n",
       "  'word_p2': 'trade',\n",
       "  'pos_n2': 'DT',\n",
       "  'pos_n1': 'JJ',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'I-NP'},\n",
       " {'word_n2': 'sharp',\n",
       "  'word_n1': 'dive',\n",
       "  'word': 'if',\n",
       "  'word_p1': 'trade',\n",
       "  'word_p2': 'figures',\n",
       "  'pos_n2': 'JJ',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'NNS',\n",
       "  'chunk_n2': 'I-NP',\n",
       "  'chunk_n1': 'I-NP'},\n",
       " {'word_n2': 'dive',\n",
       "  'word_n1': 'if',\n",
       "  'word': 'trade',\n",
       "  'word_p1': 'figures',\n",
       "  'word_p2': 'for',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'NNS',\n",
       "  'pos_p2': 'IN',\n",
       "  'chunk_n2': 'I-NP',\n",
       "  'chunk_n1': 'B-SBAR'},\n",
       " {'word_n2': 'if',\n",
       "  'word_n1': 'trade',\n",
       "  'word': 'figures',\n",
       "  'word_p1': 'for',\n",
       "  'word_p2': 'september',\n",
       "  'pos_n2': 'IN',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'NNS',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'NNP',\n",
       "  'chunk_n2': 'B-SBAR',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'trade',\n",
       "  'word_n1': 'figures',\n",
       "  'word': 'for',\n",
       "  'word_p1': 'september',\n",
       "  'word_p2': ',',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'NNS',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': ',',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'I-NP'},\n",
       " {'word_n2': 'figures',\n",
       "  'word_n1': 'for',\n",
       "  'word': 'september',\n",
       "  'word_p1': ',',\n",
       "  'word_p2': 'due',\n",
       "  'pos_n2': 'NNS',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': ',',\n",
       "  'pos_p2': 'JJ',\n",
       "  'chunk_n2': 'I-NP',\n",
       "  'chunk_n1': 'B-PP'},\n",
       " {'word_n2': 'for',\n",
       "  'word_n1': 'september',\n",
       "  'word': ',',\n",
       "  'word_p1': 'due',\n",
       "  'word_p2': 'for',\n",
       "  'pos_n2': 'IN',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': ',',\n",
       "  'pos_p1': 'JJ',\n",
       "  'pos_p2': 'IN',\n",
       "  'chunk_n2': 'B-PP',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'september',\n",
       "  'word_n1': ',',\n",
       "  'word': 'due',\n",
       "  'word_p1': 'for',\n",
       "  'word_p2': 'release',\n",
       "  'pos_n2': 'NNP',\n",
       "  'pos_n1': ',',\n",
       "  'pos': 'JJ',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'O'},\n",
       " {'word_n2': ',',\n",
       "  'word_n1': 'due',\n",
       "  'word': 'for',\n",
       "  'word_p1': 'release',\n",
       "  'word_p2': 'tomorrow',\n",
       "  'pos_n2': ',',\n",
       "  'pos_n1': 'JJ',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'O',\n",
       "  'chunk_n1': 'B-ADJP'},\n",
       " {'word_n2': 'due',\n",
       "  'word_n1': 'for',\n",
       "  'word': 'release',\n",
       "  'word_p1': 'tomorrow',\n",
       "  'word_p2': ',',\n",
       "  'pos_n2': 'JJ',\n",
       "  'pos_n1': 'IN',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': ',',\n",
       "  'chunk_n2': 'B-ADJP',\n",
       "  'chunk_n1': 'B-PP'},\n",
       " {'word_n2': 'for',\n",
       "  'word_n1': 'release',\n",
       "  'word': 'tomorrow',\n",
       "  'word_p1': ',',\n",
       "  'word_p2': 'fail',\n",
       "  'pos_n2': 'IN',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': ',',\n",
       "  'pos_p2': 'VB',\n",
       "  'chunk_n2': 'B-PP',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'release',\n",
       "  'word_n1': 'tomorrow',\n",
       "  'word': ',',\n",
       "  'word_p1': 'fail',\n",
       "  'word_p2': 'to',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': ',',\n",
       "  'pos_p1': 'VB',\n",
       "  'pos_p2': 'TO',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'tomorrow',\n",
       "  'word_n1': ',',\n",
       "  'word': 'fail',\n",
       "  'word_p1': 'to',\n",
       "  'word_p2': 'show',\n",
       "  'pos_n2': 'NN',\n",
       "  'pos_n1': ',',\n",
       "  'pos': 'VB',\n",
       "  'pos_p1': 'TO',\n",
       "  'pos_p2': 'VB',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'O'},\n",
       " {'word_n2': ',',\n",
       "  'word_n1': 'fail',\n",
       "  'word': 'to',\n",
       "  'word_p1': 'show',\n",
       "  'word_p2': 'a',\n",
       "  'pos_n2': ',',\n",
       "  'pos_n1': 'VB',\n",
       "  'pos': 'TO',\n",
       "  'pos_p1': 'VB',\n",
       "  'pos_p2': 'DT',\n",
       "  'chunk_n2': 'O',\n",
       "  'chunk_n1': 'B-VP'},\n",
       " {'word_n2': 'fail',\n",
       "  'word_n1': 'to',\n",
       "  'word': 'show',\n",
       "  'word_p1': 'a',\n",
       "  'word_p2': 'substantial',\n",
       "  'pos_n2': 'VB',\n",
       "  'pos_n1': 'TO',\n",
       "  'pos': 'VB',\n",
       "  'pos_p1': 'DT',\n",
       "  'pos_p2': 'JJ',\n",
       "  'chunk_n2': 'B-VP',\n",
       "  'chunk_n1': 'I-VP'},\n",
       " {'word_n2': 'to',\n",
       "  'word_n1': 'show',\n",
       "  'word': 'a',\n",
       "  'word_p1': 'substantial',\n",
       "  'word_p2': 'improvement',\n",
       "  'pos_n2': 'TO',\n",
       "  'pos_n1': 'VB',\n",
       "  'pos': 'DT',\n",
       "  'pos_p1': 'JJ',\n",
       "  'pos_p2': 'NN',\n",
       "  'chunk_n2': 'I-VP',\n",
       "  'chunk_n1': 'I-VP'},\n",
       " {'word_n2': 'show',\n",
       "  'word_n1': 'a',\n",
       "  'word': 'substantial',\n",
       "  'word_p1': 'improvement',\n",
       "  'word_p2': 'from',\n",
       "  'pos_n2': 'VB',\n",
       "  'pos_n1': 'DT',\n",
       "  'pos': 'JJ',\n",
       "  'pos_p1': 'NN',\n",
       "  'pos_p2': 'IN',\n",
       "  'chunk_n2': 'I-VP',\n",
       "  'chunk_n1': 'B-NP'},\n",
       " {'word_n2': 'a',\n",
       "  'word_n1': 'substantial',\n",
       "  'word': 'improvement',\n",
       "  'word_p1': 'from',\n",
       "  'word_p2': 'july',\n",
       "  'pos_n2': 'DT',\n",
       "  'pos_n1': 'JJ',\n",
       "  'pos': 'NN',\n",
       "  'pos_p1': 'IN',\n",
       "  'pos_p2': 'NNP',\n",
       "  'chunk_n2': 'B-NP',\n",
       "  'chunk_n1': 'I-NP'},\n",
       " {'word_n2': 'substantial',\n",
       "  'word_n1': 'improvement',\n",
       "  'word': 'from',\n",
       "  'word_p1': 'july',\n",
       "  'word_p2': 'and',\n",
       "  'pos_n2': 'JJ',\n",
       "  'pos_n1': 'NN',\n",
       "  'pos': 'IN',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'CC',\n",
       "  'chunk_n2': 'I-NP',\n",
       "  'chunk_n1': 'I-NP'}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now vectorize the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "classifier = linear_model.LogisticRegression()\n",
    "model = classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will finally predict the test set. We load the corpus again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'International', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Corp.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'Tulsa', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'unit', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'signed', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'tentative', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'extending', 'pos': 'VBG', 'chunk': 'B-VP'},\n",
       "  {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP'},\n",
       "  {'form': 'contract', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'with', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Co.', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'provide', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'structural', 'pos': 'JJ', 'chunk': 'B-NP'},\n",
       "  {'form': 'parts', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'Boeing', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': '747', 'pos': 'CD', 'chunk': 'I-NP'},\n",
       "  {'form': 'jetliners', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Rockwell', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'agreement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'calls', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-VP'},\n",
       "  {'form': 'supply', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': '200', 'pos': 'CD', 'chunk': 'B-NP'},\n",
       "  {'form': 'additional', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'so-called', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'shipsets', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'planes', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = read_sentences(test_file)\n",
    "test_corpus = split_rows(test_sentences, column_names)\n",
    "test_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us extract the static features from one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word_n2': 'bos',\n",
       "  'word_n1': 'bos',\n",
       "  'word': 'rockwell',\n",
       "  'word_p1': 'international',\n",
       "  'word_p2': 'corp.',\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'BOS',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'NNP'},\n",
       " {'word_n2': 'bos',\n",
       "  'word_n1': 'rockwell',\n",
       "  'word': 'international',\n",
       "  'word_p1': 'corp.',\n",
       "  'word_p2': \"'s\",\n",
       "  'pos_n2': 'BOS',\n",
       "  'pos_n1': 'NNP',\n",
       "  'pos': 'NNP',\n",
       "  'pos_p1': 'NNP',\n",
       "  'pos_p2': 'POS'}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dict, y_test = extract_features_static([test_corpus[0]], w_size, feature_names)\n",
    "X_test_dict[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\mathbf{X}\\_{\\textrm{dict}}$ is incomplete. For the prediction, we need to reinject dynamically the two previously predicted tags to have the full feature vector. Write this code here. \n",
    "\n",
    "This part is probably the most difficult of the lab. You may want to write it first for one sentence, and then for the test corpus. The prediction will take a longer time and you may want to include a progress bar with this snippet: \n",
    "```\n",
    "from tqdm import tqdm\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_dyn = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2012/2012 [09:51<00:00,  3.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "w_size = 2\n",
    "y_test_predicted_dyn = []\n",
    "from tqdm import tqdm\n",
    "for test_sentence in tqdm(test_corpus):\n",
    "    c_2 = 'BOS'\n",
    "    c_1 = 'BOS'\n",
    "    [X, y] = extract_features_sent_dyn(test_sentence, w_size, feature_names_dyn)\n",
    "    for token in X:\n",
    "        token['chunk_n2'] = c_2\n",
    "        token['chunk_n1'] = c_1\n",
    "        X_test = vec.transform([token])  # Possible to add: .toarray()\n",
    "        y_test_predicted = classifier.predict(X_test)[0]\n",
    "        c_2 = c_1\n",
    "        c_1 = y_test_predicted\n",
    "        y_test_predicted_dyn.append(y_test_predicted)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-NP', 'I-NP', 'I-NP']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted_dyn[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "inx = 0\n",
    "for sentence in test_corpus:\n",
    "    for word in sentence:\n",
    "        word['pchunk'] = y_test_predicted_dyn[inx]\n",
    "        inx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(test_corpus, keys, 'out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9265266775640221"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open('out').read().splitlines()\n",
    "res = conlleval.evaluate(lines)\n",
    "improved_ml_score = res['overall']['chunks']['evals']['f1']\n",
    "improved_ml_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an optional task, you can try to improve the score with beam search. If you know this technique, apply it using the probability output of logistic regression.\n",
    "\n",
    "With the same classifier and a beam diameter of 5, your teacher obtained 92.87."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now collect all the named entities from the training set, defined as NP chunks and starting with a `NNP` (proper noun) or a `NNPS` (proper noun, plural) tag. As an example, in the first sentence of `train_corpus`, you will extract `('September', )` and `('July', 'and', 'August')`. You will set all the tuples in a set that you will call `ne_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'form': 'Confidence', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'pound', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'is', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'widely', 'pos': 'RB', 'chunk': 'I-VP'},\n",
       "  {'form': 'expected', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'take', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'another', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'sharp', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'dive', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'if', 'pos': 'IN', 'chunk': 'B-SBAR'},\n",
       "  {'form': 'trade', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'figures', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'September', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'due', 'pos': 'JJ', 'chunk': 'B-ADJP'},\n",
       "  {'form': 'for', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'release', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'tomorrow', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': ',', 'pos': ',', 'chunk': 'O'},\n",
       "  {'form': 'fail', 'pos': 'VB', 'chunk': 'B-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'show', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'substantial', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'improvement', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'from', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'July', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'and', 'pos': 'CC', 'chunk': 'I-NP'},\n",
       "  {'form': 'August', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'near-record', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'deficits', 'pos': 'NNS', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}],\n",
       " [{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       "  {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       "  {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       "  {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       "  {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       "  {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       "  {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       "  {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       "  {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       "  {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       "  {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       "  {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       "  {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       "  {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       "  {'form': '.', 'pos': '.', 'chunk': 'O'}]]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write a two-pass procedure. For each sentence of the corpus:\n",
    "1. In the first pass, you will collect the start indices of the noun groups which are also proper nouns. For the first sentence, it will result in the list `[16, 30]`;\n",
    "2. In the second pass, you will collect the segments, starting at each index. For the first sentence, it will result in the tuples `('September',)`and `('July', 'and', 'August')`\n",
    "\n",
    "Should you have a better solution, please use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "ne_set = set()\n",
    "ind_list = list() # lista med index för träffar av NNP i början av en mening\n",
    "for sentence in train_corpus:\n",
    "    i = 0\n",
    "    for token in sentence:\n",
    "        if 'NNP' in token['pos'] and 'B-NP' in token['chunk']:\n",
    "            ind_list.append(i)  \n",
    "        i = i + 1\n",
    "\n",
    "    tuple_words = [] # lista av matches som vi ska appenda som ett tuple object\n",
    "    for index in ind_list:\n",
    "        k = 1\n",
    "        tuple_words.append(sentence[index]['form'])\n",
    "        while len(sentence)-(index+k) > 0: # för att vi ej ska få index out of bounds\n",
    "            if 'I-NP' in sentence[index + k]['chunk']:\n",
    "                tuple_words.append(sentence[index + k]['form']) \n",
    "            else:\n",
    "                k = len(sentence) + 1   # för att bryta while loopen\n",
    "            k += 1   \n",
    "        ne_set.add(tuple(tuple_words))\n",
    "        tuple_words = []\n",
    "    ind_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('mid-October',),\n",
       " ('Capcom', 'Financial'),\n",
       " ('Bank', 'Bumiputra', 'Malaysia', 'Bhd.'),\n",
       " ('Mr.', 'Krenz'),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co'),\n",
       " ('Papa',),\n",
       " ('Bally', 'Manufacturing', 'Corp.'),\n",
       " ('Greece',),\n",
       " ('Portland',),\n",
       " ('Republic',),\n",
       " ('Fur', 'Vault', 'Inc.'),\n",
       " ('Ill',),\n",
       " ('President', 'Reagan'),\n",
       " ('Ameritech',),\n",
       " ('Prudential-Bache', 'Securities'),\n",
       " ('July', '1994'),\n",
       " ('Mr.', 'Hunt'),\n",
       " ('Pearl', 'Group'),\n",
       " ('Adjustment',),\n",
       " ('John', 'C.', 'Burton'),\n",
       " ('Western', 'New', 'Mexico', 'Telephone', 'Co.'),\n",
       " ('Robert', 'Avery'),\n",
       " ('USACafes', 'Limited', 'Partnership'),\n",
       " ('Dallas', 'ad', 'friends'),\n",
       " ('Milbank', ',', 'Tweed', ',', 'Hadley', '&', 'McCloy'),\n",
       " ('Assistant', 'U.S.', 'Attorney', 'Randy', 'Mastro'),\n",
       " ('Steven', 'B.', 'Enright'),\n",
       " ('Ohioans',),\n",
       " ('John', 'Blodgett'),\n",
       " ('Sept.', '29'),\n",
       " ('Societa', 'per', 'Azioni', 'Finanziaria', 'Industria', 'Manaifatturiera'),\n",
       " ('Pinnacle', 'West', 'Capital'),\n",
       " ('U.S.', 'action'),\n",
       " ('Christopher', 'Whittington'),\n",
       " ('Queens',),\n",
       " ('American', 'Stock', 'Exchange', 'composite', 'trading'),\n",
       " ('Ashland',),\n",
       " ('Europa',),\n",
       " ('Patricia', 'Meagher', 'Davis'),\n",
       " ('Norwood',),\n",
       " ('American', 'Stock', 'Exchange'),\n",
       " ('Harvard', 'Business', 'School', 'Press'),\n",
       " ('National', 'Intergroup', 'Inc.'),\n",
       " ('Europeans',),\n",
       " ('Mr.', 'Fogg'),\n",
       " ('Richard', 'Connors'),\n",
       " ('Telerate', 'Inc.'),\n",
       " ('Singapore', 'hotel', 'and', 'property', 'magnate', 'Ong', 'Beng', 'Seng'),\n",
       " ('Amex', 'Life', 'Insurance', 'Co.'),\n",
       " ('Brown', 'Brothers', 'Harriman', '&', 'Co.'),\n",
       " ('Topix',),\n",
       " ('Nguyen', 'Ngoc'),\n",
       " ('Buckingham', 'Wile', 'Co.'),\n",
       " ('Jack', 'Guzewich'),\n",
       " ('Democratic', 'Gov.', 'James', 'Blanchard'),\n",
       " ('Theory', 'No.', '1'),\n",
       " ('Midland', 'Montagu', 'Securities', 'Inc'),\n",
       " ('Morrison', 'Knudsen'),\n",
       " ('Airlines',),\n",
       " ('May', 'Department', 'Stores', 'Co.'),\n",
       " ('Michelle', 'Van', 'Cleave'),\n",
       " ('July', '1995'),\n",
       " ('Bay', 'area', 'clients'),\n",
       " ('York',),\n",
       " ('Breeden',),\n",
       " ('Atchinson', 'County'),\n",
       " ('Apple', 'Computer', 'Inc.'),\n",
       " ('Osaka',),\n",
       " ('Wellington',),\n",
       " ('Ms.', 'Moss'),\n",
       " ('Cambridge',),\n",
       " ('Ragu', 'spaghetti', 'sauce'),\n",
       " ('Loral',),\n",
       " ('Gov.', 'George', 'Deukmejian'),\n",
       " ('Eddie', 'Bauer'),\n",
       " ('Noxell',),\n",
       " ('Scottish', 'Amicable', 'Investment', 'Managers'),\n",
       " ('Bank', 'Building', '&', 'Equipment', 'Corp.'),\n",
       " ('Amex', ',', 'not', 'I'),\n",
       " ('USACafes',),\n",
       " ('Judge', 'Louis', 'L.', 'Stanton'),\n",
       " ('Datatronic', 'AB'),\n",
       " ('Michael', 'Conway'),\n",
       " ('Rep.', 'J.', 'Dennis', 'Hastert'),\n",
       " ('R.',),\n",
       " ('Va',),\n",
       " ('Remic', 'mortgage', 'securities'),\n",
       " ('Brown-Forman',),\n",
       " ('July', '31'),\n",
       " ('Paul', 'P.', 'Aniskovich', 'Jr.'),\n",
       " ('Guber-Peters',),\n",
       " ('WALL', 'STREET'),\n",
       " ('President', 'Carlos', 'Salinas'),\n",
       " ('Jan.', '1', ',', '1990'),\n",
       " ('U.S.', 'publishing', 'trade', 'groups'),\n",
       " ('Arthur', 'Fleischer', 'Jr.'),\n",
       " ('Republican-governor\\\\/Democratic-legislature', 'lines'),\n",
       " ('House', 'Banking', 'Committee', 'members'),\n",
       " ('Ball',),\n",
       " ('Fredric', 'E.', 'Russell'),\n",
       " ('Arizona',),\n",
       " ('Chandler',),\n",
       " ('James', 'F.', 'Gero'),\n",
       " ('Senate', 'races'),\n",
       " ('Kirin', 'Brewery'),\n",
       " ('Universal', 'Studios'),\n",
       " ('Honda', 'Motor', 'Co.'),\n",
       " ('Robert', 'M.A.', 'Hirschfeld'),\n",
       " ('Federal', 'Express', 'Corp.'),\n",
       " ('Gibson', ',', 'Dunn', '&', 'Crutcher'),\n",
       " ('Nestle',),\n",
       " ('Sen.', 'William', 'Roth'),\n",
       " ('Heberto', 'Castillo'),\n",
       " ('Myrtle', 'Beach'),\n",
       " ('Appeals',),\n",
       " ('Misa', 'Manufacturing', 'Inc.'),\n",
       " ('Simon', 'Briscoe'),\n",
       " ('Adam', 'Strum'),\n",
       " ('Mr.', 'Whitten'),\n",
       " ('Bear', ',', 'Stearns'),\n",
       " ('Mr.', 'Steinhardt'),\n",
       " ('Steven', 'Milunovich'),\n",
       " ('Lotus', 'Development', 'Corp.'),\n",
       " ('Todd', 'Richter'),\n",
       " ('Omaha',),\n",
       " ('Quantum',),\n",
       " ('Shanghai',),\n",
       " ('Barbie', 'doll'),\n",
       " ('Interstate', '30'),\n",
       " ('U.S.', 'business'),\n",
       " ('Sutro',),\n",
       " ('Chemical', 'Bank'),\n",
       " ('Limited', 'Inc'),\n",
       " ('IBM', 'shares'),\n",
       " ('Louisiana', 'Pacific', 'Corp.'),\n",
       " ('Chancellor', 'Helmut', 'Kohl'),\n",
       " ('Amdura',),\n",
       " ('Dreyfus',),\n",
       " ('EGA',),\n",
       " ('Dow', 'Jones', 'Capital', 'Markets', 'Report'),\n",
       " ('Columbia',),\n",
       " ('Peter', 'Sidoti'),\n",
       " ('HUD',),\n",
       " ('Ark',),\n",
       " ('Ollie',),\n",
       " ('Kabul',),\n",
       " ('General', 'Cinema', 'Corp.'),\n",
       " ('B.A.T', 'Industries'),\n",
       " ('Ramon', 'Garcia'),\n",
       " ('Pascual', 'Duarte'),\n",
       " ('Air', 'France'),\n",
       " ('Next', 'March'),\n",
       " ('October', '18', ',', '1989'),\n",
       " ('Granges',),\n",
       " ('Nixon', ',', 'Hargrave', ',', 'Devans', '&', 'Doyle'),\n",
       " ('Viaje', 'a', 'la', 'Alcarria'),\n",
       " ('Roger', 'V.', 'Sala'),\n",
       " ('Grace', 'Perkins'),\n",
       " ('Jan.', '31', ',', '1991'),\n",
       " ('Richard', 'O.', 'Kelly', 'Sr.'),\n",
       " ('Mary', 'E.', 'Sommer'),\n",
       " ('Samuel', 'Montagu', '&', 'Co.'),\n",
       " ('Employee', 'Dishonesty'),\n",
       " ('Robert', 'Chandross'),\n",
       " ('Oppenheimer', '&', 'Co.', 'analyst', 'Myron', 'Picoult'),\n",
       " ('VALLEY', 'National', 'Corp.'),\n",
       " ('Hurricane', 'Jerry'),\n",
       " ('House', 'and', 'Senate', 'negotiators'),\n",
       " ('Control', 'Data'),\n",
       " ('Mr.', 'Regalia'),\n",
       " ('Peter', 'Scott'),\n",
       " ('New', 'Ventures'),\n",
       " ('City', 'Development', 'Ltd.'),\n",
       " ('Stephen', 'Sanders'),\n",
       " ('Saatchi', '&', 'Saatchi'),\n",
       " ('Israel',),\n",
       " ('Aug.', '31'),\n",
       " ('Ernesto', 'Vega', 'Velasco'),\n",
       " ('Gillette',),\n",
       " ('Weston', 'Capital', 'Management'),\n",
       " ('Paramount', 'Domestic', 'TV'),\n",
       " ('Stanley', 'Rose'),\n",
       " ('Peters',),\n",
       " ('Michael', 'K.', 'Evans'),\n",
       " ('Western', 'Digital', 'Corp.'),\n",
       " ('Lynch', 'Corp.'),\n",
       " ('Nick', 'Cortese'),\n",
       " ('Exabyte',),\n",
       " ('Hotel', 'Investors', 'Corp.'),\n",
       " ('Ms.', 'Slater'),\n",
       " ('Mr.', 'Gourlay'),\n",
       " ('Harry', 'B.', 'Helmsley'),\n",
       " ('Tom', 'Schlesinger'),\n",
       " ('State', 'U'),\n",
       " ('Rwanda',),\n",
       " ('Abbott', 'Laboratories'),\n",
       " ('Federal', 'Highway', 'Administrator', 'Thomas', 'Larson'),\n",
       " ('EC', 'inflation'),\n",
       " ('Gulf',),\n",
       " ('Novato',),\n",
       " ('Ms.', 'Shaevitz'),\n",
       " ('Los', 'Angeles', 'federal', 'court'),\n",
       " ('Nev.',),\n",
       " ('Calgary',),\n",
       " ('Reebok', 'chairman', 'and', 'chief', 'executive', 'officer'),\n",
       " ('Conn.',),\n",
       " ('Blondes', ',', 'cocaine', 'and', 'Corvettes'),\n",
       " ('Malaysia',),\n",
       " ('Jefferies', 'Group', 'Inc.'),\n",
       " ('Senate', 'Commerce', 'Committee', 'Chairman', 'Ernest', 'Hollings'),\n",
       " ('Xtra',),\n",
       " ('Dai-ichi', 'Securities'),\n",
       " ('Mr.', 'Rosen'),\n",
       " ('G.', 'Heileman', 'Brewing', 'Co.'),\n",
       " ('Mr.', 'Maynard'),\n",
       " ('IMA', 'Acquisition'),\n",
       " ('Jackson', 'National', 'Life', 'Insurance', 'Co.'),\n",
       " ('Hunt', 'Valley'),\n",
       " ('Shampoo',),\n",
       " ('Rochester',),\n",
       " ('American', 'Medical', 'International', 'Inc'),\n",
       " ('Garden', 'City'),\n",
       " ('National', 'Westminister', 'Bank'),\n",
       " ('Springs',),\n",
       " ('Chrysler', 'Corp'),\n",
       " ('Lebanon',),\n",
       " ('Kary', 'Moss'),\n",
       " ('CHRISTMAS', 'SHOPPERS'),\n",
       " ('Avdel',),\n",
       " ('Sunnyvale',),\n",
       " ('Ltd.',),\n",
       " ('Private', 'Eye'),\n",
       " ('Vice', 'President', 'Quayle'),\n",
       " ('Northrop', 'shares'),\n",
       " ('Charles', 'I.', 'Clough', 'Jr.'),\n",
       " ('A.', 'Schulman'),\n",
       " ('Mr.', 'Savaiko'),\n",
       " ('Drexel', 'Burnham', 'Lambert', 'Group', 'Inc.'),\n",
       " ('Yale',),\n",
       " ('Royal', 'Insurance'),\n",
       " ('Judge', 'Maloney'),\n",
       " ('Saab',),\n",
       " ('Peter', 'Lang'),\n",
       " ('Prague',),\n",
       " ('National', 'Children'),\n",
       " ('Laurence', 'Drivon'),\n",
       " ('New', 'York-based', 'John', 'Kuhns', 'and', 'Robert', 'MacDonald'),\n",
       " ('Amex', 'short', 'interest'),\n",
       " ('National', 'Restaurants'),\n",
       " ('Armstrong', 'World', 'Industries'),\n",
       " ('Judge', 'Hastings'),\n",
       " ('EC', 'growth', 'forecasts'),\n",
       " ('Brands',),\n",
       " ('CBS', 'Inc.'),\n",
       " ('American', 'Home', 'Products', 'Corp.'),\n",
       " ('God',),\n",
       " ('Mayor', 'Carolyn', 'Robinson'),\n",
       " ('Contel',),\n",
       " ('Asbury', 'Park'),\n",
       " ('McDonnell', 'Douglas', 'Corp.'),\n",
       " ('Federal', 'Paper', 'Board'),\n",
       " ('Turtle', 'Tots'),\n",
       " ('Robert', 'W.', 'Kasten'),\n",
       " ('Superstitions',),\n",
       " ('Robert', 'Bookman'),\n",
       " ('Ms.', 'Thompson'),\n",
       " ('GOP', 'state', 'Rep.', 'Stephen', 'Freind'),\n",
       " ('Whitbread', '&', 'Co.'),\n",
       " ('Corroon', '&', 'Black'),\n",
       " ('Billie', 'Jean'),\n",
       " ('Laos',),\n",
       " ('Skinner',),\n",
       " ('Fla',),\n",
       " ('Dayton', 'Hudson'),\n",
       " ('GOP', 'candidates'),\n",
       " ('Mr.', 'Amram'),\n",
       " ('Security', 'Union', 'Title', 'Insurance', 'Co'),\n",
       " ('Rorer', 'Group', 'Inc.'),\n",
       " ('Lufthansa', 'AG'),\n",
       " ('Polish', 'Prime', 'Minister', 'Tadeusz', 'Mazowiecki'),\n",
       " ('S&P', '500', 'futures'),\n",
       " ('Michael', 'Dukakis'),\n",
       " ('Houston-based', 'Stewart', 'Information', 'Services', 'Corp'),\n",
       " ('Graham', 'Beale'),\n",
       " ('Combustion', 'Engineering'),\n",
       " ('Robert', 'Willens'),\n",
       " ('Great', 'Western', 'Financial'),\n",
       " ('Conrail', 'earnings'),\n",
       " ('Prudential', 'Home', 'Mortgage', 'Co.'),\n",
       " ('Chase', 'Senior', 'Vice', 'President', 'George', 'Scandalios'),\n",
       " ('Sept.', '30', ',', '1992'),\n",
       " ('Lynn', 'Williams'),\n",
       " ('Ms.', 'Smith'),\n",
       " ('William',),\n",
       " ('Mothers',),\n",
       " ('Edward', 'L.', 'Marinaro'),\n",
       " ('Ford',),\n",
       " ('April', '30', ',', '1989'),\n",
       " ('Bristol',),\n",
       " ('Prestige', 'U.'),\n",
       " ('El', 'Salvador'),\n",
       " ('Mr.', 'Stein'),\n",
       " ('Alan', 'G.', 'Hassenfeld'),\n",
       " ('RICHMOND', 'RESIGNATIONS'),\n",
       " ('Jeff', 'C.', 'Dodd'),\n",
       " ('Friday', 'afternoon'),\n",
       " ('Feb.', '1'),\n",
       " ('June', '15'),\n",
       " ('Midland', 'Brake', 'Inc.'),\n",
       " ('California', 'prices'),\n",
       " (\"D'Arcy\", 'Masius', 'Benton', '&', 'Bowles'),\n",
       " ('Bristol', 'Federal', 'Savings', 'Bank'),\n",
       " ('Ing', '.', 'C.', 'Olivetti', '&', 'Co.'),\n",
       " ('Ernst', 'Herslow'),\n",
       " ('January', '1984'),\n",
       " ('HUD', 'loans'),\n",
       " ('Gov.', 'Kean'),\n",
       " ('Hamburg', 'and', 'Bremen'),\n",
       " ('Kimberly', 'Ann', 'Smith'),\n",
       " ('B-2', 'Stealth', 'bomber', 'research-and-development', 'revenue'),\n",
       " ('Bonn', 'and', 'Washington'),\n",
       " ('Laphroaig', 'single-malt', 'whiskey'),\n",
       " ('Halloween',),\n",
       " ('Roger',),\n",
       " ('GOP', 'Sen.', 'Specter'),\n",
       " ('U.S.', 'director'),\n",
       " ('December', '1988'),\n",
       " ('Sun', 'Alliance'),\n",
       " ('Procter', '&', 'Gamble'),\n",
       " ('Consob',),\n",
       " ('Mr.', 'Schantz'),\n",
       " ('Picop',),\n",
       " ('Kemper',),\n",
       " ('Transportation', 'Secretary', 'Samuel', 'Skinner'),\n",
       " ('FiberCom',),\n",
       " ('Speaker', 'Foley'),\n",
       " ('Arkansas',),\n",
       " ('Milan', ',', 'Paris', 'and', 'Brussels'),\n",
       " ('Oct.', '31'),\n",
       " ('Colo.',),\n",
       " ('Mr.', 'Cunningham'),\n",
       " ('Tosco', 'Corp.'),\n",
       " ('Piedmont', 'Airlines'),\n",
       " ('Chase',),\n",
       " ('Caddyshack', 'II'),\n",
       " ('Susie', 'Diamond'),\n",
       " ('Stella', 'Artois', 'beers'),\n",
       " ('Fla.',),\n",
       " ('Daimler-Benz', 'AG'),\n",
       " ('U.S.', 'exports'),\n",
       " ('Reno',),\n",
       " ('U.S.', 'productivity'),\n",
       " ('Scorpios',),\n",
       " ('Dean', 'Witter', 'Reynolds'),\n",
       " ('Paramount', 'Pictures'),\n",
       " ('Varity',),\n",
       " ('Sen.', 'John', 'Kerry'),\n",
       " ('Mr.', 'Tarter'),\n",
       " ('Lawrenceville',),\n",
       " ('April', '6', ',', '1973'),\n",
       " ('Charlotte',),\n",
       " ('Shelton',),\n",
       " ('Mr.', 'Schmedel'),\n",
       " ('Della', 'Femina', 'McNamee'),\n",
       " ('Manufacturers', 'Hanover', 'Securities', 'Corp.'),\n",
       " ('Bay', 'View', 'Capital', 'Corp'),\n",
       " ('Massachusetts', 'Attorney', 'General', 'James', 'Shannon'),\n",
       " ('Resolution', 'Funding', 'Corp.'),\n",
       " ('Assistant', 'U.S.', 'Attorney', 'Terry', 'Hart'),\n",
       " ('U.S.', 'intelligence'),\n",
       " ('James', 'Carder'),\n",
       " ('Alexandra', 'Armstrong', 'Advisors', 'Inc.'),\n",
       " ('Mr.', 'Clough'),\n",
       " ('U.S.', 'government', 'agencies'),\n",
       " ('Rockford',),\n",
       " ('Mr.', 'Lowell'),\n",
       " ('U.K.', 'economist'),\n",
       " ('Aug.', '28'),\n",
       " ('Alfred', 'Frawley'),\n",
       " ('Keihin', 'Electric', 'Express', 'Railway', 'Co'),\n",
       " ('Security', 'Pacific', 'Corp.'),\n",
       " ('Southfield',),\n",
       " ('National', 'Security', 'Affairs'),\n",
       " ('Poodle', 'Springs', 'high', 'life'),\n",
       " ('Deloitte', ',', 'Haskins', '&', 'Sells'),\n",
       " ('World', 'War', 'II'),\n",
       " ('Senate', 'Democrats'),\n",
       " ('Mr.', 'Schulman'),\n",
       " ('Jeffrey', 'Schaefer'),\n",
       " ('Philip', 'B.', 'Morris'),\n",
       " ('October', '1990'),\n",
       " ('Frankfurt', ',', 'Zurich', ',', 'Paris', 'and', 'Amsterdam'),\n",
       " ('Drexel', 'Burnham', 'Lambert'),\n",
       " ('Johannesburg',),\n",
       " ('GE', 'technology'),\n",
       " ('Harvard',),\n",
       " ('Mr.', 'Lackey'),\n",
       " ('Columbia', 'Pictures', 'Entertainment', 'Inc.'),\n",
       " ('Michael', 'Fay'),\n",
       " ('P&G',),\n",
       " ('Costa', 'Rica'),\n",
       " ('Alan', \"D'Agosto\"),\n",
       " ('Republican', 'freeholders'),\n",
       " ('Oct',),\n",
       " ('Guber', 'Peters', 'Entertainment', 'Co.'),\n",
       " ('Scandinavian', 'Airlines', 'System'),\n",
       " ('Valley', 'National'),\n",
       " ('Messrs.', 'Kelly', 'and', 'Newcomb'),\n",
       " ('Oklahoma', 'official', 'Robert', 'Fulton'),\n",
       " ('Pine', 'Bluff'),\n",
       " ('William', 'Kelly'),\n",
       " ('IRS', 'computers'),\n",
       " ('Golden', 'West', 'Financial', 'Corp.'),\n",
       " ('Philippe', 'Gras'),\n",
       " ('Khmer', 'Rouge', 'camps'),\n",
       " ('Dec.', '31', ',', '1990'),\n",
       " ('Powder', 'Springs'),\n",
       " ('Gulf', 'Canada', 'Resources', 'Ltd.'),\n",
       " ('Southwest', 'lawmakers'),\n",
       " ('AT&T', 'managers'),\n",
       " ('Chevy',),\n",
       " ('Sept.', '28', ',', '1990'),\n",
       " ('John', 'L.', 'Murray'),\n",
       " ('Adlai', 'Stevenson'),\n",
       " ('PaineWebber', 'Group', 'Inc.'),\n",
       " ('DEC', 'shares'),\n",
       " ('Big', 'Board', 'data'),\n",
       " ('San', 'Mateo'),\n",
       " ('Maalox', 'advertising', 'and', 'promotion'),\n",
       " ('St.', 'Johns', 'River', 'Water', 'Management', 'District'),\n",
       " ('Sam', 'Houston'),\n",
       " ('Midland', 'Bank', 'PLC'),\n",
       " ('Martin', 'J.', '``', 'Hoot', \"''\", 'McInerney'),\n",
       " ('Chambers',),\n",
       " ('Dr.', 'Bailit'),\n",
       " ('Oberhausen',),\n",
       " ('Management', 'and', 'unions'),\n",
       " ('Fine', 'Line'),\n",
       " ('U.S.', 'units'),\n",
       " ('Brazil',),\n",
       " ('Judge', 'Abramson'),\n",
       " ('Cherokee',),\n",
       " ('Peter', 'G.', 'Diamandis'),\n",
       " ('Sasea', 'Holding', 'S.A.'),\n",
       " ('Pieter', 'Bruwer'),\n",
       " ('Mr.', 'Noriega'),\n",
       " ('Representatives',),\n",
       " ('Holiday', 'debt'),\n",
       " ('First', 'Meridian', 'Corp.'),\n",
       " ('Hunter', 'Environmental', 'Services', 'Inc.'),\n",
       " ('Ann', 'Adams', 'Webster'),\n",
       " ('Citicorp', 'and', 'Chase'),\n",
       " ('Treasury', 'bills'),\n",
       " ('IBM',),\n",
       " ('Big', 'Board', 'composite', 'trading'),\n",
       " ('Warner', 'Communications', 'Inc'),\n",
       " ('Bruce', 'Hoyt'),\n",
       " ('LBO', 'debt'),\n",
       " ('Coca-Cola', 'Co.'),\n",
       " ('Donald', 'Trinen'),\n",
       " ('Parenthood',),\n",
       " ('IBM', 'equipment'),\n",
       " ('Barris',),\n",
       " ('Czechoslovakia',),\n",
       " ('Berkshire', 'Hathaway'),\n",
       " ('WASHINGTON',),\n",
       " ('Winchester',),\n",
       " ('Houston', 'Chronicle', 'columnist', 'Jim', 'Barlow'),\n",
       " ('LifeSpan', 'Inc.'),\n",
       " ('Helmsley', 'management'),\n",
       " ('Chris-Craft', 'Industries'),\n",
       " ('Congress',),\n",
       " ('English',),\n",
       " ('Sen.', 'Packwood'),\n",
       " ('Mr.', 'Pignatelli'),\n",
       " ('Mr.', 'Dell'),\n",
       " ('Aetna', 'Life', '&', 'Casualty', 'Insurance', 'Co.'),\n",
       " ('Drexel', 'Burnham'),\n",
       " ('Chris', 'Dillow'),\n",
       " ('Noranda',),\n",
       " ('Mr.', 'Bockris'),\n",
       " ('Robert', 'Volland'),\n",
       " ('Dean', 'Witter', 'Reynolds', 'Inc'),\n",
       " ('Hendrik', 'Jr'),\n",
       " ('Merrill', 'Lynch', 'Research'),\n",
       " ('Westwood', 'Group'),\n",
       " ('Hugo',),\n",
       " ('Telesystems',),\n",
       " ('Esso',),\n",
       " ('Denmark',),\n",
       " ('Bell', 'Atlantic', 'Corp.'),\n",
       " ('American', 'Express', 'card', 'charge', 'volume'),\n",
       " ('Inland',),\n",
       " ('Mr.', 'Premner'),\n",
       " ('Santa', 'Fe', 'Pacific', 'Corp.'),\n",
       " ('Dell', 'Computer', 'Corp.'),\n",
       " ('Hongkong', '&', 'Shanghai', 'Banking', 'Corp.'),\n",
       " ('Raymond', 'P.', 'Keenan'),\n",
       " ('MIT',),\n",
       " ('Long', 'Island'),\n",
       " ('Automated', 'teller', 'machine', 'operations'),\n",
       " ('Pa.',),\n",
       " ('Manhattan', 'National', 'Corp.'),\n",
       " ('St.', 'Petersburg'),\n",
       " ('California', 'state', 'and', 'local', 'municipal', 'debt'),\n",
       " ('ADN',),\n",
       " ('Indian', 'Oil', 'Corp'),\n",
       " ('February',),\n",
       " ('Mr.', 'Blodgett'),\n",
       " ('Average', 'daily', 'trading', 'volume'),\n",
       " ('National', 'Taxpayers', 'Union'),\n",
       " ('Prof.', 'James', 'Tobin'),\n",
       " ('Mr.', 'Luthringshausen'),\n",
       " ('American', 'Brands', 'shares'),\n",
       " ('Rep.', 'Mary', 'Rose', 'Oakar'),\n",
       " ('Mr.', 'Ross'),\n",
       " ('Texans',),\n",
       " ('Manufacturers', 'Hanover', 'Trust', 'Co.'),\n",
       " ('Arfeen', 'International'),\n",
       " ('Fill-Or-Kill', 'Order'),\n",
       " ('John', 'H.', 'Cammack'),\n",
       " ('Addington',),\n",
       " ('IAFP',),\n",
       " ('Federal', 'Reserve', 'banks'),\n",
       " ('AGS', 'Computers'),\n",
       " ('Bart', 'Giamatti'),\n",
       " ('Stuart-James', 'Co.'),\n",
       " ('Prudential-Bache',),\n",
       " ('October', '19', ',', '1989'),\n",
       " ('Petco',),\n",
       " ('Union', 'Pacific', 'Corp', '.', 'third-quarter', 'net', 'income'),\n",
       " ('Indiana', 'Senator', 'Dan', 'Coats'),\n",
       " ('Sun', 'Microsystems', 'Inc.'),\n",
       " ('B.A.T', 'Industries', 'PLC'),\n",
       " ('April', '1987'),\n",
       " ('LMEYER',),\n",
       " ('Misawa', 'Homes'),\n",
       " ('Mr.', 'Dreyer'),\n",
       " ('New', 'Mexico'),\n",
       " ('Philips',),\n",
       " ('Germany',),\n",
       " ('Honeywell',),\n",
       " ('Symbol', 'Technologies'),\n",
       " ('John', 'H.', 'OBrion', ',', 'Jr.'),\n",
       " ('Tokyo-based', 'Pioneer'),\n",
       " ('John', 'E.', 'Hayes', 'Jr.'),\n",
       " ('Seoul',),\n",
       " ('Mr.', 'Rohs'),\n",
       " ('Doman', 'Industries', 'Ltd.'),\n",
       " ('John', 'Gargan'),\n",
       " ('Stinson', 'Beach'),\n",
       " ('U.S.', 'takeover-stock', 'speculators'),\n",
       " ('Northern', 'California'),\n",
       " ('Houston',),\n",
       " ('Tela',),\n",
       " ('Renault',),\n",
       " ('Freddie', 'Mac', 'and', 'Fannie', 'Mae', 'pools'),\n",
       " ('Chase', 'Manhattan', 'Bank'),\n",
       " ('N.', 'Nomura', '&', 'Co'),\n",
       " ('Human', 'nature'),\n",
       " ('Menlo', 'Park'),\n",
       " ('Jeffrey', 'E.', 'Levin'),\n",
       " ('John', 'Zumbrunn'),\n",
       " ('Fuji', 'technology'),\n",
       " ('Republic', 'New', 'York', 'Corp.'),\n",
       " ('Delaware',),\n",
       " ('Lep', 'Group', 'PLC'),\n",
       " ('Buffalo',),\n",
       " ('Amway', 'sales'),\n",
       " ('General', 'Electric', 'Co.'),\n",
       " ('Apple', 'Computer'),\n",
       " ('Beverly', 'Hills'),\n",
       " ('American', 'International', 'Group', 'Inc.'),\n",
       " ('Mr.', 'Sala'),\n",
       " ('Salomon',),\n",
       " ('South', 'Carolina'),\n",
       " ('January',),\n",
       " ('Sir', 'James'),\n",
       " ('Florida', 'state', 'court'),\n",
       " ('Steep', 'Rock', 'Resources', 'Inc'),\n",
       " ('Camden',),\n",
       " ('Guber', 'Peters', 'Entertainment', 'Co'),\n",
       " ('Republican',),\n",
       " ('July', '31', ',', '1989'),\n",
       " ('Metropolitan', 'Life', 'Insurance', 'Co.'),\n",
       " ('Equitec', 'Chairman', 'Richard', 'L.', 'Saalfeld'),\n",
       " ('Mr.', 'Conway'),\n",
       " ('House', 'Speaker', 'James', 'Wright'),\n",
       " ('Trinova',),\n",
       " ('New', 'Jersey'),\n",
       " ('Infotechnology', 'Inc.'),\n",
       " ('Paul', \"D'Arcy\"),\n",
       " ('Mr.', 'Lang'),\n",
       " ('North', 'American', 'readers'),\n",
       " ('Leo', 'J.', 'Shapiro', '&', 'Associates'),\n",
       " ('Republican', 'Sen.', 'William', 'Cohen'),\n",
       " ('Anton', 'Amon'),\n",
       " ('Mr.', 'Dole'),\n",
       " ('Judith', 'Valente'),\n",
       " ('Neal', 'Litvack'),\n",
       " ('Mr.', 'Coats'),\n",
       " ('Quotron',),\n",
       " ('Atlanta',),\n",
       " ('Lazard', 'Freres', '&', 'Co.'),\n",
       " ('Los', 'Angeles-based', 'Ticor'),\n",
       " ('New', 'Canaan'),\n",
       " ('Universal', 'Foods', 'Corp.'),\n",
       " ('CML', 'Group', 'Inc.'),\n",
       " ('H&R', 'Block', 'shares'),\n",
       " ('Rep.', 'Thomas', 'Downey'),\n",
       " ('Tom', 'Schumacher'),\n",
       " ('Chambers', 'Development', 'Co.'),\n",
       " ('Tuesday',),\n",
       " ('Ho', 'Chi', 'Minh', 'City', 'party', 'secretary'),\n",
       " ('American', 'Express', 'Bank', 'earnings'),\n",
       " ('Japanese-Americans',),\n",
       " ('Sun', 'Life'),\n",
       " ('Education',),\n",
       " ('Jack', 'Greenberg'),\n",
       " ('David', 'Semmel'),\n",
       " ('Don', 'Borgeson'),\n",
       " ('Conn',),\n",
       " ('Madrid', 'life'),\n",
       " ('Philip', 'Ross'),\n",
       " ('Chicago',),\n",
       " ('Chicago', 'Title', '&', 'Trust'),\n",
       " ('TransCanada', 'PipeLines', 'Ltd.'),\n",
       " ('Golden', 'Nugget', 'Inc.'),\n",
       " ('Va.',),\n",
       " ('Copyright',),\n",
       " ('Mr.', 'Kim'),\n",
       " ('Foreign', 'Ministry', 'spokesman', 'Li', 'Zhaoxing'),\n",
       " ('Blunt', 'Ellis'),\n",
       " ('Toledo',),\n",
       " ('James', 'B.', 'Lee'),\n",
       " ('Mental', 'Health'),\n",
       " ('Bolinas',),\n",
       " ('Sony',),\n",
       " ('Rep.', 'Stephen', 'Neal'),\n",
       " ('Northern', 'California', 'home', 'prices'),\n",
       " ('Fernando', 'Volio'),\n",
       " ('George', 'A.', 'Wiegers'),\n",
       " ('United', 'Air'),\n",
       " ('New',\n",
       "  'York',\n",
       "  'real',\n",
       "  'estate',\n",
       "  'brokerage',\n",
       "  'Edward',\n",
       "  'S.',\n",
       "  'Gordon',\n",
       "  'Co.'),\n",
       " ('Alberta',),\n",
       " ('Jiangsu', 'and', 'Zhejiang'),\n",
       " ('J.P.', 'Morgan', '&', 'Co.'),\n",
       " ('Quotron', 'Systems', 'Inc.'),\n",
       " ('Government', 'National', 'Mortgage', 'Association', '9', '%', 'securities'),\n",
       " ('Hugh', 'Ray'),\n",
       " ('Valley', 'Stream'),\n",
       " ('Coastal', 'Corp.'),\n",
       " ('Merkur',),\n",
       " ('Caldwell', 'Butler'),\n",
       " ('Canadian', 'Pacific', 'and', 'Soo', 'Line', 'tracks'),\n",
       " ('Aug.', '1'),\n",
       " ('Banco', 'Popular'),\n",
       " ('Lone', 'Star'),\n",
       " ('East', 'Germans'),\n",
       " ('Jack', 'Purnick'),\n",
       " ('Nov.', '1'),\n",
       " ('Oct.', '25'),\n",
       " ('Federal', 'Home', 'Loan', 'Mortgage', 'Corp.', '9', '%', 'securities'),\n",
       " ('Trade',),\n",
       " ('Burrillville',),\n",
       " ('Murray', 'Riese'),\n",
       " ('Denise', 'McDonald'),\n",
       " ('Mr.', 'Cawthorn'),\n",
       " ('USX',),\n",
       " ('Warner',),\n",
       " ('Joseph', 'B.', 'Vittoria'),\n",
       " ('NBC',),\n",
       " ('Ashland', 'Oil', 'Inc.'),\n",
       " ('April',),\n",
       " ('U.K.', 'brokerage', 'James', 'Capel', '&', 'Co'),\n",
       " ('Trinova', 'Corp.'),\n",
       " ('Sen.', 'Bob', 'Packwood'),\n",
       " ('Jacobs', 'Engineering', 'Group'),\n",
       " ('February', '1974'),\n",
       " ('Continental',),\n",
       " ('Contract', 'Appeals'),\n",
       " ('Metromedia', 'Co.'),\n",
       " ('Pinnacle',),\n",
       " ('Arafat',),\n",
       " ('El', 'Paso'),\n",
       " ('Mao', 'Tse-tung'),\n",
       " ('Tuesday', 'night'),\n",
       " ('Nov.', '3'),\n",
       " ('Manhattan', 'National', 'Life', 'Insurance', 'Co'),\n",
       " ('Ciba-Geigy',),\n",
       " ('Doman',),\n",
       " ('JKD',),\n",
       " ('Tokyo', 'market', 'participants'),\n",
       " ('Mr.', 'Morrissey'),\n",
       " ('David', 'Rahill'),\n",
       " ('President', 'Salinas'),\n",
       " ('Nov.', '27', ',', '1994'),\n",
       " ('Guadalajara',),\n",
       " ('First', 'Boston', 'Corp'),\n",
       " ('Rep.', 'Chalmers', 'Wylie'),\n",
       " ('Salvadoran', 'President', 'Alfredo', 'Cristiani'),\n",
       " ('El', 'Espectador', 'journalists', 'and', 'staff'),\n",
       " ('IBM', 'and', 'MCA'),\n",
       " ('Daniel', 'James'),\n",
       " ('San', 'Rafael'),\n",
       " ('Qintex',),\n",
       " ('Winter', 'Haven'),\n",
       " ('Robert', 'Delaney'),\n",
       " ('UAL', 'Corp.'),\n",
       " ('Miss.',),\n",
       " ('East', 'Berlin'),\n",
       " ('Leon', 'J.', 'Warshaw'),\n",
       " ('Jacki', 'Ragan'),\n",
       " ('Columbus',),\n",
       " ('Mr.', 'Litvack'),\n",
       " ('DDB', 'Needham'),\n",
       " ('AC&R', 'ADVERTISING'),\n",
       " ('Mr.', 'Vyas'),\n",
       " ('Mr.', 'Batchelder'),\n",
       " ('Yankee', 'Group'),\n",
       " ('March', '1'),\n",
       " ('U.S.', 'Census', 'data'),\n",
       " ('Campeau', 'Corp.'),\n",
       " ('Oct.', '30'),\n",
       " ('Merill', 'Lynch', 'International', 'Ltd'),\n",
       " ('Air', 'Force', 'research'),\n",
       " ('Mayor', 'Agnos'),\n",
       " ('Nov.', '10'),\n",
       " ('Ford', 'trucks'),\n",
       " ('Last', 'Christmas', 'Day'),\n",
       " ('Colgate', 'toothpaste'),\n",
       " ('Tandem',),\n",
       " ('Jerry', 'Junkins'),\n",
       " ('James', 'Smith'),\n",
       " ('Mr.', 'Isler'),\n",
       " ('Carat',),\n",
       " ('Basf',),\n",
       " ('Yamaichi', 'International'),\n",
       " ('NBC', 'affiliate', 'WAVE'),\n",
       " ('Cornell', 'University'),\n",
       " ('Aegon', 'N.V'),\n",
       " ('Cambodia',),\n",
       " ('Society',),\n",
       " ('Pemex', 'managers'),\n",
       " ('South', 'Africa'),\n",
       " ('Mr.', 'Shannon'),\n",
       " ('Chernobyl',),\n",
       " ('Damascus',),\n",
       " ('SIERRA', 'TUCSON', 'Cos.'),\n",
       " ('Wall', 'Street', 'brokerage', 'firms'),\n",
       " ('Mr.', 'Renzas'),\n",
       " ('Chancellor', 'Kohl'),\n",
       " ('Budget',),\n",
       " ('Sumner', 'M.', 'Redstone'),\n",
       " ('Webster\\\\/Eagle', 'Bancorp', 'Inc.'),\n",
       " ('Mitsubishi', 'Eclipse', 'cars'),\n",
       " ('Kinney', 'and', 'Foot', 'Locker', 'shoe', 'stores'),\n",
       " ('Aetna', 'Life', '&', 'Casualty'),\n",
       " ('Greiner', 'Engineering'),\n",
       " ('Isola',),\n",
       " ('Southern', 'New', 'England', 'Telecommunications'),\n",
       " ('Greenwich',),\n",
       " ('Thomas', 'Kurlak'),\n",
       " ('Semel', '&', 'Co.'),\n",
       " ('Hustead',),\n",
       " ('Harry', 'Rossi'),\n",
       " ('America', 'Inc.'),\n",
       " ('Tandy',),\n",
       " ('Bonwit',\n",
       "  'Teller',\n",
       "  'and',\n",
       "  'B.',\n",
       "  'Altman',\n",
       "  'parent',\n",
       "  'L.J.',\n",
       "  'Hooker',\n",
       "  'Corp.'),\n",
       " ('Commons',),\n",
       " ('Cessna',),\n",
       " ('Rights',),\n",
       " ('West', 'Germany'),\n",
       " ('NV', 'DSM'),\n",
       " ('September', 'durable', 'goods', 'orders'),\n",
       " ('Maumee',),\n",
       " ('Amway', 'products'),\n",
       " ('Brookline',),\n",
       " ('Nippon', 'Yusen', 'Kaisha'),\n",
       " ('Oracle', 'Systems'),\n",
       " ('William', 'Vizas'),\n",
       " ('Mikhail', 'Gorbachev'),\n",
       " ('Fighting',),\n",
       " ('Fremont',),\n",
       " ('Amoco', 'Oil', 'Co.'),\n",
       " ('Highland', 'Valley', 'mine'),\n",
       " ('Dec.', '20'),\n",
       " ('Mr.', 'McClelland'),\n",
       " ('U.S.', 'economic', 'or', 'military', 'aid'),\n",
       " ('Canadian', 'Pacific', 'Ltd.'),\n",
       " ('GATT', 'membership'),\n",
       " ('RD',),\n",
       " ('Pacific', 'Daylight', 'Time'),\n",
       " ('Oct.', '5'),\n",
       " ('AMR', 'shareholders'),\n",
       " ('Network', 'Computing', 'System'),\n",
       " ('Mr.', 'Cockburn'),\n",
       " ('Africa',),\n",
       " ('B-2', 'development', 'dollars'),\n",
       " ('Shearson', 'Lehman', 'Hutton', 'Inc.'),\n",
       " ('William', 'Yankus'),\n",
       " ('Robert', 'Lekberg'),\n",
       " ('Warner', 'executives'),\n",
       " ('Veronis', 'Suhler'),\n",
       " ('DDB', 'Needham', 'executives'),\n",
       " ('Coke', 'and', 'Pepsi'),\n",
       " ('Robert', 'Cawthorn'),\n",
       " ('Safeco',),\n",
       " ('Albert', 'Engelken'),\n",
       " ('Switzerland',),\n",
       " ('Ron', 'Littleboy'),\n",
       " ('U.S.', 'products'),\n",
       " ('May', '1987'),\n",
       " ('Capital', 'Holding'),\n",
       " ('San', 'Francisco-based', 'Pacific', 'Telesis'),\n",
       " ('Odeon', 'owner', 'Keith', 'McNally'),\n",
       " ('Ignazio', 'J.', 'Ruvolo'),\n",
       " ('Hollywood', 'manpower'),\n",
       " ('Burton', 'Lee'),\n",
       " ('USAir',),\n",
       " ('Guber',),\n",
       " ('Stokely',),\n",
       " ('Chiron', 'Corp'),\n",
       " ('Carter', 'Hawley', 'Hale', 'Stores', 'Inc.'),\n",
       " ('Official', 'Airline', 'Guides'),\n",
       " ('Galveston-Houston', 'Co.'),\n",
       " ('Houston-based', 'First', 'City', 'Bancorp.'),\n",
       " ('Oct.', '27', ',', '1999'),\n",
       " ('Watson', '&', 'Hughey', 'Co.'),\n",
       " ('Mr.', 'Manion'),\n",
       " ('Don', 'Browne'),\n",
       " ('Wang',),\n",
       " ('NCNB', 'Corp.'),\n",
       " ('Anthony', 'Richmond-Watson'),\n",
       " ('Fletcher', 'Challenge', 'Canada'),\n",
       " ('Shell',),\n",
       " ('Steve', 'Kloves'),\n",
       " ('Geneva',),\n",
       " ('N.Y',),\n",
       " ('Life',),\n",
       " ('Mr.', 'Ballhaus'),\n",
       " ('Pakistan',),\n",
       " ('OTC', 'trading'),\n",
       " ('Federal', 'Home', 'Loan', 'Mortgage', 'Corp.'),\n",
       " ('New', 'Ventures', 'Fund', 'Inc.'),\n",
       " ('Agricola',),\n",
       " ('Afrikanerdom',),\n",
       " ('Mr.', 'Bush'),\n",
       " ('Alex', '.', 'Brown', '&', 'Sons'),\n",
       " ('First', 'Options'),\n",
       " ('Venus',),\n",
       " ('Leo',),\n",
       " ('Peter', 'S.', 'Willmott'),\n",
       " ('Texas', 'A&M', 'University'),\n",
       " ('Mr.', 'Honecker'),\n",
       " ('Mr.', 'Dillow'),\n",
       " ('Mitsubishi', 'Rayon'),\n",
       " ('Alexander', '&', 'Alexander', 'Services'),\n",
       " ('Lone', 'Star', 'Beer'),\n",
       " ('Abbott', 'Park'),\n",
       " ('Units',),\n",
       " ('Stanley', 'Chesley'),\n",
       " ('Combustion', 'Engineering', 'Inc.'),\n",
       " ('Nestle', 'Korea'),\n",
       " ('Eurocom', 'WCRS', 'Della', 'Femina', 'Ball', 'Ltd.'),\n",
       " ('Utah',),\n",
       " ('Reinsurance',),\n",
       " ('George', 'Gourlay'),\n",
       " ('Santa', 'Cruz'),\n",
       " ('Mr.', 'Hoyt'),\n",
       " ('Mr.', 'Daly'),\n",
       " ('Baxter',),\n",
       " ('Mr.', 'Fiorini'),\n",
       " ('Zacks', 'Investment', 'Research'),\n",
       " ('October', '1988'),\n",
       " ('Third', 'World', 'loans'),\n",
       " ('Fame',),\n",
       " ('CONTINENTAL', 'Airlines'),\n",
       " ('Journal', 'readers'),\n",
       " ('Land',),\n",
       " ('Sunday', 'River'),\n",
       " ('Jack', 'Grubman'),\n",
       " ('Delaware', 'Chancery', 'Court'),\n",
       " ('Delaware', 'County'),\n",
       " ('J.', 'Chandler', 'Peterson'),\n",
       " ('U.S.', 'Treasury', 'issues'),\n",
       " ('Mr.', 'De', 'Palma'),\n",
       " ('Hot', 'Wheels', 'cars'),\n",
       " ('Ahmad', 'Jibril'),\n",
       " ('Dun', '&', 'Bradstreet', 'Corp.'),\n",
       " ('Becton', ',', 'Dickinson', '&', 'Co.'),\n",
       " ('Richard', 'J.', 'Pinola'),\n",
       " ('Chevron', 'Corp.'),\n",
       " ('U.S.', 'credit', 'services'),\n",
       " ('U.K.',),\n",
       " ('Mr.', 'Williams'),\n",
       " ('Guy', 'Hempel'),\n",
       " ('Taurus', 'or', 'Escort'),\n",
       " ('Dow', 'Jones', 'futures', 'index'),\n",
       " ('San', 'Juan'),\n",
       " ('Westmoreland',),\n",
       " ('Timothy', 'Kochis'),\n",
       " ('U.S.', 'companies'),\n",
       " ('Chuck', 'Digate'),\n",
       " ('Mr.', 'Amon'),\n",
       " ('International', 'Proteins'),\n",
       " ('Wall', 'Street', \"'s\"),\n",
       " ('Sanjay', 'Joshi'),\n",
       " ('N.J.',),\n",
       " ('Shearson', 'Lehman', 'Hutton', 'Treasury', 'index'),\n",
       " ('Texas', 'A&M'),\n",
       " ('N.H.',),\n",
       " ('Ralph', 'T.', 'Linsley'),\n",
       " ('VOA', 'transmissions'),\n",
       " ('January', 'platinum'),\n",
       " ('Empire-Berol',),\n",
       " ('Jack', 'Smalling'),\n",
       " ('Guinness',),\n",
       " ('Bank', 'Bumiputra'),\n",
       " ('Princeton',),\n",
       " ('Los', 'Angeles-based', 'H.F.', 'Ahmanson'),\n",
       " ('Colorado', 'Springs'),\n",
       " ('New', 'York', 'Co.'),\n",
       " ('American', 'Telephone', '&', 'Telegraph', 'Co.'),\n",
       " ('Mr.', 'Byrum'),\n",
       " ('SEC', 'Chairman', 'Richard', 'Breeden'),\n",
       " ('Peter', 'and', 'Jon'),\n",
       " ('Main', 'Street'),\n",
       " ('Romania',),\n",
       " ('Management',),\n",
       " ('John', 'Pagones'),\n",
       " ('Premium',\n",
       "  'Government',\n",
       "  'National',\n",
       "  'Mortgage',\n",
       "  'Association',\n",
       "  'securities'),\n",
       " ('James', 'Baker'),\n",
       " ('Silicon', 'Graphics', 'Inc.'),\n",
       " ('Robert', 'M.', 'Gintel'),\n",
       " ('Mr.', 'Mattausch'),\n",
       " ('Specific-Time', 'Order'),\n",
       " ('Tony', 'Adamski'),\n",
       " ('Missouri', 'farmer', 'Blake', 'Hurst'),\n",
       " ('Mr.', 'Haines'),\n",
       " ('WTI',),\n",
       " ('Kevin', 'Logan'),\n",
       " ('December', '1995'),\n",
       " ('Jaguar', 'shareholders'),\n",
       " ('McGraw-Hill', 'Inc.'),\n",
       " ('Chase', 'Manhattan'),\n",
       " ('Toyota', 'Corolla'),\n",
       " ('Mrs.', 'Aquino'),\n",
       " ('George', 'Gray'),\n",
       " ('Dona', 'Rosa'),\n",
       " ('Sperry', 'Corp'),\n",
       " ('Daimler-Benz',),\n",
       " ('National', 'Steel', 'Corp'),\n",
       " ('Ajinomoto',),\n",
       " ('Lily',),\n",
       " ('July', '29'),\n",
       " ('United', 'Telecommunications'),\n",
       " ('Illinois', 'Minerals', 'Co.'),\n",
       " ('Occidental', 'Petroleum', 'Corp.'),\n",
       " ('Columbus', ',', 'Ohio'),\n",
       " ('Mr.', 'Bradford'),\n",
       " ('Pasquale', 'Pignatelli'),\n",
       " ('Pioneer', 'Electronic', 'Corp.'),\n",
       " ('Hochiminh', 'City', 'Food', 'Co.'),\n",
       " ('Martin', 'Fleischmann'),\n",
       " ('Cipher', 'Data', 'Products', 'Inc.'),\n",
       " ('Rep.', 'James', 'Oberstar'),\n",
       " ('Newark',),\n",
       " ('Keizaikai',),\n",
       " ('MBB',),\n",
       " ('Aug.', '9'),\n",
       " ('Nov.', '9'),\n",
       " ('IMA',),\n",
       " ('GTE', 'spokesman', 'Thomas', 'Mattausch'),\n",
       " ('Turnaround', 'Letter'),\n",
       " ('Jones', 'Medical', 'Industries', 'Inc.'),\n",
       " ('Lynch',),\n",
       " ('Avdel', 'PLC'),\n",
       " ('Joel', 'Popkin'),\n",
       " ('Du', 'Pont', 'Co'),\n",
       " ('Mr.', 'Checchi'),\n",
       " ('Treasury', 'Secretary', 'Brady'),\n",
       " ('Hartford',),\n",
       " ('BankAmerica', 'common', 'stock'),\n",
       " ('Yoshiaki', 'Mitsuoka'),\n",
       " ('Chevrolet', 'Prizm'),\n",
       " ('Compassion',),\n",
       " ('First', 'Interstate'),\n",
       " ('Wednesday', 'or', 'Thursday'),\n",
       " ('Mr.', 'Katz'),\n",
       " ('Newport', 'Beach', 'telemarketers'),\n",
       " ('Ferdinand', 'Marcos'),\n",
       " ('General', 'Electric'),\n",
       " ('Palms',),\n",
       " ('Hurricane', 'Hugo'),\n",
       " ('Bear', ',', 'Stearns', '&', 'Co.'),\n",
       " ('Mr.', 'Mahe'),\n",
       " ('mare-COOR',),\n",
       " ('Mr.', 'Logan'),\n",
       " ('Chemical', 'Banking', 'Corp.'),\n",
       " ...}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my first inserted cell\n",
    "ne_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4348"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ne_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mid-October',),\n",
       " ('Capcom', 'Financial'),\n",
       " ('Bank', 'Bumiputra', 'Malaysia', 'Bhd.'),\n",
       " ('Mr.', 'Krenz'),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co'),\n",
       " ('Papa',),\n",
       " ('Bally', 'Manufacturing', 'Corp.'),\n",
       " ('Greece',),\n",
       " ('Portland',),\n",
       " ('Republic',)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ne_set)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a small set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the subsequent experiments faster, you will limit the dataset to the entities starting with letter _K_. I chose this letter, because it corresponded to one of the smallest sets. You will call the resulting set: `ne_small_set`. Feel free to use the full set after you have completed this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "ne_small_set = [] # borde vara ett set MEN det blir ett set pga det vi hämtar från är ett set\n",
    "for tp in ne_set:\n",
    "    if tp[0].startswith('K'):\n",
    "        ne_small_set.append(tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kidder', ',', 'Peabody', '&', 'Co'),\n",
       " ('Kirin', 'Brewery'),\n",
       " ('Kabul',),\n",
       " ('Kary', 'Moss'),\n",
       " ('Kimberly', 'Ann', 'Smith'),\n",
       " ('Kemper',),\n",
       " ('Keihin', 'Electric', 'Express', 'Railway', 'Co'),\n",
       " ('Khmer', 'Rouge', 'camps'),\n",
       " ('Kinney', 'and', 'Foot', 'Locker', 'shoe', 'stores'),\n",
       " ('Kevin', 'Logan'),\n",
       " ('Keizaikai',),\n",
       " ('Kansas',),\n",
       " ('Kringle', 'fares'),\n",
       " ('Kuala', 'Lumpur'),\n",
       " ('Kobe', 'Steel'),\n",
       " ('Kathryn', 'McGrath'),\n",
       " ('Kremlin', 'wrangling'),\n",
       " ('Knoxville',),\n",
       " ('Kasler', 'Corp.'),\n",
       " ('Karen', 'Olshan'),\n",
       " ('Kleinwort', 'Benson', 'North', 'America'),\n",
       " ('Kathie', 'Roberts'),\n",
       " ('KLM', 'Royal', 'Dutch', 'Airlines'),\n",
       " ('Kentucky', 'Fried', 'Chicken', 'stores'),\n",
       " ('Kenneth', 'A.', 'Eldred'),\n",
       " ('Kirin',),\n",
       " ('Keefe', ',', 'Bruyette', '&', 'Woods', 'Inc.'),\n",
       " ('Kent', 'Neal'),\n",
       " ('Keizaikai', 'Corp.'),\n",
       " ('KIM',),\n",
       " ('Khost',),\n",
       " ('Kathie', 'Huff'),\n",
       " ('Kidder', 'Peabody'),\n",
       " ('Kentucky',),\n",
       " ('Kumagai-Gumi',),\n",
       " ('Katonah',),\n",
       " ('Kim', 'Schwartz'),\n",
       " ('Kean', 'forces'),\n",
       " ('Kursk', 'and', 'Smolensk'),\n",
       " ('Kleinwort', 'Benson', 'Government', 'Securities', 'Inc.'),\n",
       " ('Kollmorgen',),\n",
       " ('Kenneth', 'Abraham'),\n",
       " ('Ke', 'Zaishuo'),\n",
       " ('Kodak',),\n",
       " ('Kidder', ',', 'Peabody', '&', 'Co.'),\n",
       " ('Kurt', 'Hager'),\n",
       " ('Kenneth', 'H.', 'Olsen'),\n",
       " ('Kenneth', 'T.', 'Rosen'),\n",
       " ('Krenz',),\n",
       " ('Kyocera',),\n",
       " ('Kansas', 'and', 'Texas'),\n",
       " ('Kobe', 'Steel', 'Ltd.'),\n",
       " ('Kajima',),\n",
       " ('Kwek', 'Hong', 'Png'),\n",
       " ('Kurds',),\n",
       " ('KTXL',),\n",
       " ('Kawasaki', 'Steel'),\n",
       " ('Kansas', 'Power'),\n",
       " ('Kitchen',),\n",
       " ('Kraft', 'General', 'Foods'),\n",
       " ('Ky.',),\n",
       " ('Kacy', 'McClelland'),\n",
       " ('Kroger', 'Co'),\n",
       " ('Kathe', 'Dillmann'),\n",
       " ('Kathy', 'Stanwick'),\n",
       " ('Kate', 'Michelman'),\n",
       " ('Ky',),\n",
       " ('Ko', 'Shioya'),\n",
       " ('Keith', 'Mulrooney'),\n",
       " ('K', 'mart', 'Corp.', 'Chairman', 'Joseph', 'E.', 'Antonini'),\n",
       " ('Kenton',),\n",
       " ('King', 'Broadcasting', 'Co.'),\n",
       " ('KPMG', 'Peat', 'Marwick'),\n",
       " ('Keith', 'L.', 'Fogg'),\n",
       " ('K-H', 'Corp.'),\n",
       " ('Kaitaia',)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_small_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving the entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now implement a simple method to find the named entities from the previous exercise in Wikipedia and Wikidata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, look at a few entities in your set and find:\n",
    "1. a few entities that you think are in wikipedia, \n",
    "2. entities that will not be in wikipedia, and \n",
    "3. entities that you think are ambiguous: An entity that may correspond to two or more things. \n",
    "\n",
    "You will describe your findings in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to lookup entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the function below and try to understand what it means. You will describe it in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_lookup(ner, base_url='https://en.wikipedia.org/wiki/'):\n",
    "    try:\n",
    "        url_en = base_url + ' '.join(ner)\n",
    "        html_doc = requests.get(url_en).text\n",
    "        parse_tree = bs4.BeautifulSoup(html_doc, 'html.parser')\n",
    "        entity_id = parse_tree.find(\"a\", {\"accesskey\": \"g\"})['href']\n",
    "        head_id, entity_id = os.path.split(entity_id)\n",
    "        return entity_id\n",
    "    except:\n",
    "        pass\n",
    "        # print('Not found in: ', base_url)\n",
    "    entity_id = 'UNK'\n",
    "    return entity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to run the lookup and keep the resolved entities (only the resolved entities). You will call it `ne_ids_en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "def ne_ids_en(set_of_entities):\n",
    "    ne_ids_en = []\n",
    "    for entity in set_of_entities:\n",
    "        article = wikipedia_lookup(entity)\n",
    "        if 'UNK' not in article:\n",
    "            print(entity)\n",
    "            ne_ids_en.append(tuple([entity, article]))\n",
    "    return ne_ids_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kirin', 'Brewery')\n",
      "('Kabul',)\n",
      "('Kemper',)\n",
      "('Kansas',)\n",
      "('Kuala', 'Lumpur')\n",
      "('Kobe', 'Steel')\n",
      "('Knoxville',)\n",
      "('KLM', 'Royal', 'Dutch', 'Airlines')\n",
      "('Kirin',)\n",
      "('KIM',)\n",
      "('Khost',)\n",
      "('Kidder', 'Peabody')\n",
      "('Kentucky',)\n",
      "('Katonah',)\n",
      "('Kenneth', 'Abraham')\n",
      "('Kodak',)\n",
      "('Kurt', 'Hager')\n",
      "('Kenneth', 'H.', 'Olsen')\n",
      "('Krenz',)\n",
      "('Kyocera',)\n",
      "('Kajima',)\n",
      "('Kwek', 'Hong', 'Png')\n",
      "('Kurds',)\n",
      "('KTXL',)\n",
      "('Kawasaki', 'Steel')\n",
      "('Kitchen',)\n",
      "('Kraft', 'General', 'Foods')\n",
      "('Ky.',)\n",
      "('Kate', 'Michelman')\n",
      "('Ky',)\n",
      "('Kenton',)\n",
      "('KPMG', 'Peat', 'Marwick')\n",
      "('Kaitaia',)\n"
     ]
    }
   ],
   "source": [
    "en_entities = ne_ids_en(ne_small_set)\n",
    "#print(en_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, entities need a confirmation. You will apply the resolution with the Swedish wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def ne_ids_sv(set_of_entities):\n",
    "    ne_ids_sv = []\n",
    "    for entity in set_of_entities:\n",
    "        article = wikipedia_lookup(entity, 'https://sv.wikipedia.org/wiki/')\n",
    "        if 'UNK' not in article:\n",
    "            ne_ids_sv.append(tuple([entity, article]))\n",
    "    return ne_ids_sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('KIM',), 'Q224736'), (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'), (('Kabul',), 'Q5838'), (('Kansas',), 'Q1558'), (('Kenton',), 'Q358393'), (('Kentucky',), 'Q1603'), (('Khost',), 'Q386682'), (('Kirin',), 'Q297659'), (('Kitchen',), 'Q952431'), (('Knoxville',), 'Q232749'), (('Kodak',), 'Q486269'), (('Kraft', 'General', 'Foods'), 'Q12857502'), (('Kroger', 'Co'), 'Q153417'), (('Kuala', 'Lumpur'), 'Q1865')]\n"
     ]
    }
   ],
   "source": [
    "# sv_entities = ne_ids_sv(ne_small_set)\n",
    "print(sv_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will compute the intersection of the two sets. You will assign it to a list that you will sort and that you will call: `confirmed_ne_en_sv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "confirmed_ne_en_sv = []\n",
    "en_entities.sort()\n",
    "sv_entities.sort() \n",
    "for tp in range(len(sv_entities)):\n",
    "    if sv_entities[tp] in en_entities:\n",
    "        confirmed_ne_en_sv.append(sv_entities[tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('KIM',), 'Q224736'), (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'), (('Kabul',), 'Q5838'), (('Kansas',), 'Q1558'), (('Kenton',), 'Q358393'), (('Kentucky',), 'Q1603'), (('Khost',), 'Q386682'), (('Kirin',), 'Q297659'), (('Kodak',), 'Q486269'), (('Kuala', 'Lumpur'), 'Q1865')]\n"
     ]
    }
   ],
   "source": [
    "print(confirmed_ne_en_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first items in your list should look like:\n",
    "```\n",
    "[(('KIM',), 'Q224736'),\n",
    " (('KLM', 'Royal', 'Dutch', 'Airlines'), 'Q181912'),\n",
    " ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have written all the code and run all the cells, fill in your ID and as well as the name of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "STIL_ID = [\"elt15jli\", \"elt15afa\"] # Write your stil ids as a list\n",
    "CURRENT_NOTEBOOK_PATH = os.path.join(os.getcwd(), \n",
    "                                     \"4-chunker_solution.ipynb\") # Write the name of your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission code will send your answer. It consists of the baseline score, the improved machine-learning score, and the confirmed entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"baseline_score\": 0.770671072299583, \"improved_ml_score\": 0.9265266775640221, \"confirmed_ne_en_sv\": [[[\"KIM\"], \"Q224736\"], [[\"KLM\", \"Royal\", \"Dutch\", \"Airlines\"], \"Q181912\"], [[\"Kabul\"], \"Q5838\"], [[\"Kansas\"], \"Q1558\"], [[\"Kenton\"], \"Q358393\"], [[\"Kentucky\"], \"Q1603\"], [[\"Khost\"], \"Q386682\"], [[\"Kirin\"], \"Q297659\"], [[\"Kodak\"], \"Q486269\"], [[\"Kuala\", \"Lumpur\"], \"Q1865\"]]}'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "ANSWER = json.dumps({'baseline_score': baseline_score,\n",
    "                    'improved_ml_score': improved_ml_score,\n",
    "                    'confirmed_ne_en_sv': confirmed_ne_en_sv})\n",
    "ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the moment of truth:\n",
    "1. Save your notebook and\n",
    "2. Run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_NOTEBOOK_PATH = CURRENT_NOTEBOOK_PATH + \".submission.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "ASSIGNMENT = 4\n",
    "API_KEY = \"f581ba347babfea0b8f2c74a3a6776a7\"\n",
    "\n",
    "# Copy and compress current notebook\n",
    "with bz2.open(SUBMISSION_NOTEBOOK_PATH, mode=\"wb\") as fout:\n",
    "    with open(CURRENT_NOTEBOOK_PATH, \"rb\") as fin:\n",
    "        fout.write(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msg': None,\n",
       " 'status': 'correct',\n",
       " 'signature': '53946850872936b40142496ac6c074ab50bad4dfd417c0a14248b1ddbd23caf75a5ae84f4c9e15b862ca8bacf6d5a93754030c5050a9d4e60e9528f76ded0aee',\n",
       " 'submission_id': '6089800d-33e9-4fe7-9814-389ab93f6219'}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.post(\"https://vilde.cs.lth.se/edan20checker/submit\", \n",
    "                    files={\"notebook_file\": open(SUBMISSION_NOTEBOOK_PATH, \"rb\")}, \n",
    "                    data={\n",
    "                        \"stil_id\": STIL_ID,\n",
    "                        \"assignment\": ASSIGNMENT,\n",
    "                        \"answer\": ANSWER,\n",
    "                        \"api_key\": API_KEY,\n",
    "                    },\n",
    "               verify=True)\n",
    "\n",
    "# from IPython.display import display, JSON\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the article, <a href=\"https://www.aclweb.org/anthology/C18-1139\"><i>Contextual String\n",
    "            Embeddings for Sequence Labeling</i></a> by Akbik et al. (2018)\n",
    "            and you will outline the main differences between their system and yours. A LSTM is a type of\n",
    "            recurrent neural network, while CRF is a sort of beam search. You will tell the performance\n",
    "            they reach on the corpus you used in this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
